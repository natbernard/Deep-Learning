{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Learning with Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine learning algorithms come up with rules for processin data bu learning from inputs and corresponding outputs.\n",
    "- Machine learning algorithms consist of automatically finding appropriate transformations to turn the input data to useful representations that get us closer to the expected output.\n",
    "- *Learning*, in the context of machine learning, describes an automatic search process for better representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fundamentals of Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subset of machine learning, mathematical framework for learning representations from data.\n",
    "- Emphasizes a different take on learning representations from data, that is, learning from subsequent layers of increasingly meaningful representations.(Extracting the useful representations is like feature engineering).\n",
    "- The neural network transforms the input data into representations that are increasingly different from the original data and increasingly informative about the final result.\n",
    "- You can think of a deep network as a multistage information-distillation operation, where information goes through successive filters and comes out increasingly purified (that is, useful with regard to some task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The layers in a neural network transform data into useful representations that bring the model closer to the output.\n",
    "- Transformations applied to the input data are parameterized by the layer's weights.\n",
    "- The initial weights assigned to a layer are essentially random, tehrefore initial transformations are random and consequently result in a high loss finction.\n",
    "- To be able to control the resulting output, the neural network makes predictions from the data, compares the predictions with the true targets and calculates a loss score through a loss function(objective function).\n",
    "- The loss function calculates the distance between the predictions and the true targets.\n",
    "- The loss score is the used to adjust thee weights to values that result in a lower low score(predictions closer to the true targets). This is the work of the optimizer which conducts back propagation, where weights are constantly adjusted to get a low loss score. (Training loop).\n",
    "- A network with a minimal loss is one for which the outputs are as close as they can be to the targets: a trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The reason why deep learning is more effective than previous machine learning models is beacuse it allows for an incremental, layer-by-layer way in which increasingly complex representations are developed, and the fact that these intermediate incremental representations are learned jointly, each layer being updated to follow both the representational needs of the layer above and the needs of the layer below.\n",
    "- Neural networks automate feature engineering, the model gets to learn all features jointly and when one feature is adjusted all other features dependent on it automatically adjust with the change all with the similar goal of improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The core building block of a neural network is the layer.\n",
    "- Layers extract representations out of the data fed into them.\n",
    "- Most of deep learning consists of chaining together simple layers that will implement a form of progressive data distillation. A deep-learning model is like a sieve for data processing, made of a succession of increasingly refined data filters—the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Representations for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Tensors* are containers for data, usually numerical data. Like a numpy array, with several axis\n",
    "> ~ Vector data— 2D tensors of shape (samples, features)<br>\n",
    "  ~ Timeseries data or sequence data— 3D tensors of shape (samples, timesteps, features)<br>\n",
    "  ~ Images— 4D tensors of shape (samples, height, width, channels) or (samples, channels, height, width)<br>\n",
    "  ~ Video— 5D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width)<br>\n",
    "- *Scalars* are tensors with one number (or scalar tensor, or 0-dimensionaltensor, or 0D tensor)\n",
    "- *Vector* is an array of numbers. Like a numpy array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Weights in a deep learning model are first assigned random numbers, which is called *random initialization*, then they are gradually adjusted based on the feedback signal.\n",
    "- Below is the deep learning training loop:\n",
    "> Draw a batch of training samples x and corresponding targets y.<br>\n",
    "Run the network on x (a step called the forward pass) to obtain predictions y_pred.<br>\n",
    "Compute the loss of the network on the batch, a measure of the mismatch between y_pred and y.<br>\n",
    "Update all weights of the network in a way that slightly reduces the loss on this batch.\n",
    "- Gradient based optimization involves using the magnitude of the derivative (slope) to determine whether or not to increase or decrease weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mini-batch stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Draw a batch of training samples x and corresponding targets y .\n",
    "- Run the network on x to obtain predictions y_pred .\n",
    "- Compute the loss of the network on the batch, a measure of the mismatch between y_pred and y .\n",
    "- Compute the gradient of the loss with regard to the network’s parameters (a backward pass).\n",
    "- Move the parameters a little in the opposite direction from the gradient—for example W -= step * gradient —thus reducing the loss on the batch a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vector data (2D, samples and features) usually processed by densely connected layers (Dense)\n",
    "- Sequence data (3D, samples, features and timesteps) processed by recurrent layers(eg LSTM)\n",
    "- Image data (4D) processed by 2D convolution layers (Conv2D)\n",
    "- Consequent layers have to be compatible, input of a layer should match the output of previous layer\n",
    "- In an input layer, you first specify the shape of the output tensor, then input shape\n",
    "- You dont specify the shape of the subsequent layers as they automatically know that the input shape is the output shape of the previous layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choosing the right loss/ objective function for a problem is important as it should correlate with the success of the task at hand for your model to effectively minimize it, and so that the model serves the purpose you're creating it for\n",
    "- For instance, you’ll use binary crossentropy for a two-class classification problem, categorical crossentropy for a many-class classification problem, mean-squared error for a regression problem, connectionist temporal classification ( CTC )for a sequence-learning problem, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having 16 hidden units means the weight matrix W will have shape (input_dimension,16) : the dot product with W will project the input data onto a 16-dimensional representation space (and then you’ll add the bias vector b and apply the relu operation). You can intuitively understand the dimensionality of your representation space as “how much freedom you’re allowing the network to have when learning internal representations.” Having more hidden units (a higher-dimensional representation space) allows your network to learn more-complex representations, but it makes the network more computationally expensive and may lead to learning unwanted patterns (patterns that will improve performance on the training data but not on the test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why are activation functions important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Without an activation function like relu (also called a non-linearity), the Dense layer would consist of two linear operations—a dot product and an addition:\n",
    "output = dot(W, input) + b\n",
    "- So the layer could only learn linear transformations (affine transformations) of the input data: the hypothesis space of the layer would be the set of all possible linear transformations of the input data into a 16-dimensional space. Such a hypothesis space is too restricted and wouldn’t benefit from multiple layers of representations, because a deep stack of linear layers would still implement a linear operation: adding more layers wouldn’t extend the hypothesis space.\n",
    "- In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. \n",
    "- relu is themost popular activation function in deep learning, but there are many other candidates, which all come with similarly strange names: prelu, elu, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a binary classification problem (two output classes), your network should end with a Dense layer with one unit and a sigmoid activation: the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
    "- With such a scalar sigmoid output on a binary classification problem, the loss function you should use is binary_crossentropy .\n",
    "- The rmsprop optimizer is generally a good enough choice, whatever your problem. That’s one less thing for you to worry about.\n",
    "- As they get better on their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data they’ve never seen before. Be sure to always monitor performance on data that is outside of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a multiclass classification problem, the loss function categorical_crossentropy expects labels to be in a categorical format, done usin the to_categorical() function.\n",
    "- When you use np.asarray() to convert your labels to integer labels, use sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general, the less training data you have, the worse overfitting will be, and using a small network is one way to mitigate overfitting.\n",
    "- In scalar regression, (a regression where you’re trying to predict a single continuous value), applying an activation function would constrain the range the output can take; for instance, if you applied a sigmoid activation function to the last layer,the network could only learn to predict values between 0 and 1.\n",
    "- When you have a small set of data, instead of slitting to get a validation set you can use the k-fold cross-validation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression models:\n",
    "- Regression is done using different loss functions than what we used for classification. Mean squared error ( MSE ) is a loss function commonly used for regres-\n",
    "sion.\n",
    "- Similarly, evaluation metrics to be used for regression differ from those used for classification; naturally, the concept of accuracy doesn’t apply for regression. A common regression metric is mean absolute error ( MAE ).\n",
    "- When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.\n",
    "- When there is little data available, using K-fold validation is a great way to reliably evaluate a model.\n",
    "- When little training data is available, it’s preferable to use a small network with few hidden layers (typically only one or two), in order to avoid severe overfitting.\n",
    "- If your data is divided into many categories, you may cause information\n",
    "bottlenecks if you make the intermediate layers too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fundamentals of Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting is the main obstacke to creating machine learning models that generalize well on never-before-seen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In model evaluation, it is important to split your data into three parts; the training set, validation set and the test set. \n",
    "- Using the validation data during hyperparameter and parameter tuning and eventually the test data to test model performance prevents overfitting and information leaks which can result in a model that performs artificially well on validation data and poorly on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic Evaluation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Simple hold-out validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Involves simply setting apart a fraction of your data to be used as the test set. Train on the rest of the data then evaluate on the test set.\n",
    "- Tuning your model on the test set will lead to information leaks, and you should therefore also set aside a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Flaw: if little data is available, then your validation and test sets may contain too few samples to be statistically representative of the data at hand. This is easy to recognize: if different random\n",
    "shuffling rounds of the data before splitting end up yielding very different measures of model performance, then you’re having this issue. K-fold validation and iterated K -fold validation are two ways to address this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *K-Fold Validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split your data into K partitions of equal size. \n",
    "- For each partition i , train a model on the remaining K – 1 partitions, and evaluate it on partition i .\n",
    "- Your final score is then the averages of the K scores obtained. \n",
    "- This method is helpful when the performance of your model shows significant variance based on your train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Iterated K-Fold validation with shuffling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consists of applying K -fold validation multiple times, shuffling the data every time before splitting it K ways. The final score is the average of the\n",
    "scores obtained at each run of K -fold validation. \n",
    "- Note that you end up training and evaluating P × K models (where P is the number of iterations you use), which can very expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to consider when choosing an evaluation method:\n",
    "- Data Representativeness (randomly shuffle data before splitting, unless its time series data)\n",
    "- Arrow of time\n",
    "- Redundancy in data (ensure data points don't appear in both the train and evaluation data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing, feature engineering and feature learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcd2c53e9736f599d911168c349fa93ea4da0fc59fb270cea2da0241c3d0904b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
